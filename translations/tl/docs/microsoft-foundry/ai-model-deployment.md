# Pag-deploy ng AI Model gamit ang Azure Developer CLI

**Pag-navigate sa Kabanata:**
- **üìö Home ng Kurso**: [AZD Para sa Mga Baguhan](../../README.md)
- **üìñ Kasalukuyang Kabanata**: Kabanata 2 - AI-First Development
- **‚¨ÖÔ∏è Nakaraan**: [Microsoft Foundry Integration](microsoft-foundry-integration.md)
- **‚û°Ô∏è Susunod**: [AI Workshop Lab](ai-workshop-lab.md)
- **üöÄ Susunod na Kabanata**: [Kabanata 3: Configuration](../getting-started/configuration.md)

Ang gabay na ito ay nagbibigay ng detalyadong mga tagubilin para sa pag-deploy ng mga AI model gamit ang mga AZD template, mula sa pagpili ng modelo hanggang sa mga pattern ng deployment sa produksyon.

## Talaan ng Nilalaman

- [Istratehiya sa Pagpili ng Modelo](../../../../docs/microsoft-foundry)
- [AZD Configuration para sa AI Models](../../../../docs/microsoft-foundry)
- [Mga Pattern ng Deployment](../../../../docs/microsoft-foundry)
- [Pamamahala ng Modelo](../../../../docs/microsoft-foundry)
- [Mga Pagsasaalang-alang sa Produksyon](../../../../docs/microsoft-foundry)
- [Pagsubaybay at Pagmamasid](../../../../docs/microsoft-foundry)

## Istratehiya sa Pagpili ng Modelo

### Azure OpenAI Models

Piliin ang tamang modelo para sa iyong use case:

```yaml
# azure.yaml - Model configuration
services:
  ai-service:
    project: ./infra
    host: containerapp
    config:
      AZURE_OPENAI_MODELS: |
        [
          {
            "name": "gpt-4o-mini",
            "version": "2024-07-18",
            "deployment": "gpt-4o-mini",
            "capacity": 10,
            "format": "OpenAI"
          },
          {
            "name": "text-embedding-ada-002",
            "version": "2",
            "deployment": "text-embedding-ada-002", 
            "capacity": 30,
            "format": "OpenAI"
          }
        ]
```

### Pagpaplano ng Kapasidad ng Modelo

| Uri ng Modelo | Gamit | Inirerekomendang Kapasidad | Pagsasaalang-alang sa Gastos |
|---------------|-------|---------------------------|-----------------------------|
| GPT-4o-mini | Chat, Q&A | 10-50 TPM | Makatipid para sa karamihan ng mga workload |
| GPT-4 | Masalimuot na pangangatwiran | 20-100 TPM | Mas mataas ang gastos, gamitin para sa premium na mga tampok |
| Text-embedding-ada-002 | Search, RAG | 30-120 TPM | Mahalaga para sa semantic search |
| Whisper | Speech-to-text | 10-50 TPM | Para sa mga audio processing workload |

## AZD Configuration para sa AI Models

### Bicep Template Configuration

Gumawa ng mga deployment ng modelo gamit ang mga Bicep template:

```bicep
// infra/main.bicep
@description('OpenAI model deployments')
param openAiModelDeployments array = [
  {
    name: 'gpt-4o-mini'
    model: {
      format: 'OpenAI'
      name: 'gpt-4o-mini'
      version: '2024-07-18'
    }
    sku: {
      name: 'Standard'
      capacity: 10
    }
  }
  {
    name: 'text-embedding-ada-002'
    model: {
      format: 'OpenAI'
      name: 'text-embedding-ada-002'
      version: '2'
    }
    sku: {
      name: 'Standard'
      capacity: 30
    }
  }
]

resource openAi 'Microsoft.CognitiveServices/accounts@2023-05-01' = {
  name: openAiAccountName
  location: location
  kind: 'OpenAI'
  properties: {
    customSubDomainName: openAiAccountName
    networkAcls: {
      defaultAction: 'Allow'
    }
    publicNetworkAccess: 'Enabled'
  }
  sku: {
    name: 'S0'
  }
}

@batchSize(1)
resource deployment 'Microsoft.CognitiveServices/accounts/deployments@2023-05-01' = [for deployment in openAiModelDeployments: {
  parent: openAi
  name: deployment.name
  properties: {
    model: deployment.model
  }
  sku: deployment.sku
}]
```

### Mga Environment Variable

I-configure ang environment ng iyong application:

```bash
# .env na konfigurasyon
AZURE_OPENAI_ENDPOINT=https://your-openai-resource.openai.azure.com/
AZURE_OPENAI_API_VERSION=2024-02-15-preview
AZURE_OPENAI_CHAT_DEPLOYMENT=gpt-4o-mini
AZURE_OPENAI_EMBED_DEPLOYMENT=text-embedding-ada-002
```

## Mga Pattern ng Deployment

### Pattern 1: Single-Region Deployment

```yaml
# azure.yaml - Single region
services:
  ai-app:
    project: ./src
    host: containerapp
    config:
      AZURE_OPENAI_ENDPOINT: ${AZURE_OPENAI_ENDPOINT}
      AZURE_OPENAI_CHAT_DEPLOYMENT: gpt-4o-mini
```

Pinakamainam para sa:
- Development at testing
- Mga application para sa isang market
- Pag-optimize ng gastos

### Pattern 2: Multi-Region Deployment

```bicep
// Multi-region deployment
param regions array = ['eastus2', 'westus2', 'francecentral']

resource openAiMultiRegion 'Microsoft.CognitiveServices/accounts@2023-05-01' = [for region in regions: {
  name: '${openAiAccountName}-${region}'
  location: region
  // ... configuration
}]
```

Pinakamainam para sa:
- Mga global na application
- Mga kinakailangan sa mataas na availability
- Pamamahagi ng load

### Pattern 3: Hybrid Deployment

Pagsamahin ang Azure OpenAI sa iba pang mga AI service:

```bicep
// Hybrid AI services
resource cognitiveServices 'Microsoft.CognitiveServices/accounts@2023-05-01' = {
  name: cognitiveServicesName
  location: location
  kind: 'CognitiveServices'
  properties: {
    customSubDomainName: cognitiveServicesName
  }
  sku: {
    name: 'S0'
  }
}

resource documentIntelligence 'Microsoft.CognitiveServices/accounts@2023-05-01' = {
  name: documentIntelligenceName
  location: location
  kind: 'FormRecognizer'
  properties: {
    customSubDomainName: documentIntelligenceName
  }
  sku: {
    name: 'S0'
  }
}
```

## Pamamahala ng Modelo

### Version Control

Subaybayan ang mga bersyon ng modelo sa iyong AZD configuration:

```json
{
  "models": {
    "chat": {
      "name": "gpt-4o-mini",
      "version": "2024-07-18",
      "fallback": "gpt-35-turbo"
    },
    "embedding": {
      "name": "text-embedding-ada-002",
      "version": "2"
    }
  }
}
```

### Mga Update sa Modelo

Gamitin ang AZD hooks para sa mga update sa modelo:

```bash
#!/bin/bash
# hooks/predeploy.sh

echo "Checking model availability..."
az cognitiveservices account list-models \
  --name $AZURE_OPENAI_ACCOUNT_NAME \
  --resource-group $AZURE_RESOURCE_GROUP \
  --query "[?name=='gpt-4o-mini']"
```

### A/B Testing

I-deploy ang maraming bersyon ng modelo:

```bicep
param enableABTesting bool = false

resource chatDeployment 'Microsoft.CognitiveServices/accounts/deployments@2023-05-01' = {
  parent: openAi
  name: 'gpt-4o-mini-${enableABTesting ? 'v1' : 'prod'}'
  properties: {
    model: {
      format: 'OpenAI'
      name: 'gpt-4o-mini'
      version: '2024-07-18'
    }
  }
  sku: {
    name: 'Standard'
    capacity: enableABTesting ? 5 : 10
  }
}
```

## Mga Pagsasaalang-alang sa Produksyon

### Pagpaplano ng Kapasidad

Kalkulahin ang kinakailangang kapasidad batay sa mga pattern ng paggamit:

```python
# Halimbawa ng pagkalkula ng kapasidad
def calculate_required_capacity(
    requests_per_minute: int,
    avg_prompt_tokens: int,
    avg_completion_tokens: int,
    safety_margin: float = 0.2
) -> int:
    """Calculate required TPM capacity."""
    total_tokens_per_request = avg_prompt_tokens + avg_completion_tokens
    total_tpm = requests_per_minute * total_tokens_per_request
    return int(total_tpm * (1 + safety_margin))

# Halimbawa ng paggamit
required_capacity = calculate_required_capacity(
    requests_per_minute=10,
    avg_prompt_tokens=500,
    avg_completion_tokens=200,
    safety_margin=0.3
)
print(f"Required capacity: {required_capacity} TPM")
```

### Auto-scaling Configuration

I-configure ang auto-scaling para sa Container Apps:

```bicep
resource containerApp 'Microsoft.App/containerApps@2024-03-01' = {
  name: containerAppName
  properties: {
    template: {
      scale: {
        minReplicas: 1
        maxReplicas: 10
        rules: [
          {
            name: 'http-rule'
            http: {
              metadata: {
                concurrentRequests: '10'
              }
            }
          }
          {
            name: 'cpu-rule'
            custom: {
              type: 'cpu'
              metadata: {
                type: 'Utilization'
                value: '70'
              }
            }
          }
        ]
      }
    }
  }
}
```

### Pag-optimize ng Gastos

Magpatupad ng mga kontrol sa gastos:

```bicep
@description('Enable cost management alerts')
param enableCostAlerts bool = true

resource budgetAlert 'Microsoft.Consumption/budgets@2023-05-01' = if (enableCostAlerts) {
  name: 'ai-budget-alert'
  properties: {
    timePeriod: {
      startDate: '2024-01-01'
      endDate: '2024-12-31'
    }
    timeGrain: 'Monthly'
    amount: 1000
    category: 'Cost'
    notifications: {
      Actual_GreaterThan_80_Percent: {
        enabled: true
        operator: 'GreaterThan'
        threshold: 80
        contactEmails: [
          'admin@yourcompany.com'
        ]
      }
    }
  }
}
```

## Pagsubaybay at Pagmamasid

### Application Insights Integration

I-configure ang pagsubaybay para sa mga AI workload:

```bicep
resource applicationInsights 'Microsoft.Insights/components@2020-02-02' = {
  name: applicationInsightsName
  location: location
  kind: 'web'
  properties: {
    Application_Type: 'web'
    WorkspaceResourceId: logAnalyticsWorkspace.id
  }
}

// Custom metrics for AI models
resource aiMetrics 'Microsoft.Insights/components/analyticsItems@2020-02-02' = {
  parent: applicationInsights
  name: 'ai-model-metrics'
  properties: {
    content: '''
      customEvents
      | where name == "AI_Model_Request"
      | extend model = tostring(customDimensions.model)
      | extend tokens = toint(customDimensions.tokens)
      | extend latency = toint(customDimensions.latency_ms)
      | summarize 
          requests = count(),
          avg_tokens = avg(tokens),
          avg_latency = avg(latency)
        by model, bin(timestamp, 5m)
    '''
    type: 'query'
    scope: 'shared'
  }
}
```

### Custom Metrics

Subaybayan ang mga AI-specific na metrics:

```python
# Pasadyang telemetry para sa mga modelo ng AI
import logging
from applicationinsights import TelemetryClient

class AITelemetry:
    def __init__(self, instrumentation_key: str):
        self.client = TelemetryClient(instrumentation_key)
    
    def track_model_request(self, model: str, tokens: int, latency_ms: int, success: bool):
        """Track AI model request metrics."""
        self.client.track_event(
            'AI_Model_Request',
            {
                'model': model,
                'tokens': str(tokens),
                'latency_ms': str(latency_ms),
                'success': str(success)
            }
        )
        
    def track_model_error(self, model: str, error_type: str, error_message: str):
        """Track AI model errors."""
        self.client.track_exception(
            type=error_type,
            value=error_message,
            properties={
                'model': model,
                'component': 'ai_model'
            }
        )
```

### Health Checks

Magpatupad ng health monitoring para sa AI service:

```python
# Mga endpoint para sa pagsusuri ng kalusugan
from fastapi import FastAPI, HTTPException
import httpx

app = FastAPI()

@app.get("/health/ai-models")
async def check_ai_models():
    """Check AI model availability."""
    try:
        # Subukan ang koneksyon sa OpenAI
        async with httpx.AsyncClient() as client:
            response = await client.get(
                f"{AZURE_OPENAI_ENDPOINT}/openai/deployments",
                headers={"api-key": AZURE_OPENAI_API_KEY}
            )
            
        if response.status_code == 200:
            return {"status": "healthy", "models": response.json()}
        else:
            raise HTTPException(status_code=503, detail="AI models unavailable")
            
    except Exception as e:
        raise HTTPException(status_code=503, detail=f"Health check failed: {str(e)}")
```

## Mga Susunod na Hakbang

1. **Suriin ang [Microsoft Foundry Integration Guide](microsoft-foundry-integration.md)** para sa mga pattern ng service integration
2. **Kumpletuhin ang [AI Workshop Lab](ai-workshop-lab.md)** para sa hands-on na karanasan
3. **Ipatupad ang [Production AI Practices](production-ai-practices.md)** para sa mga enterprise deployment
4. **Galugarin ang [AI Troubleshooting Guide](../troubleshooting/ai-troubleshooting.md)** para sa mga karaniwang isyu

## Mga Resources

- [Azure OpenAI Model Availability](https://learn.microsoft.com/azure/ai-services/openai/concepts/models)
- [Azure Developer CLI Documentation](https://learn.microsoft.com/azure/developer/azure-developer-cli/)
- [Container Apps Scaling](https://learn.microsoft.com/azure/container-apps/scale-app)
- [AI Model Cost Optimization](https://learn.microsoft.com/azure/ai-services/openai/how-to/manage-costs)

---

**Pag-navigate sa Kabanata:**
- **üìö Home ng Kurso**: [AZD Para sa Mga Baguhan](../../README.md)
- **üìñ Kasalukuyang Kabanata**: Kabanata 2 - AI-First Development
- **‚¨ÖÔ∏è Nakaraan**: [Microsoft Foundry Integration](microsoft-foundry-integration.md)
- **‚û°Ô∏è Susunod**: [AI Workshop Lab](ai-workshop-lab.md)
- **üöÄ Susunod na Kabanata**: [Kabanata 3: Configuration](../getting-started/configuration.md)

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**Paunawa**:  
Ang dokumentong ito ay isinalin gamit ang AI translation service na [Co-op Translator](https://github.com/Azure/co-op-translator). Bagama't sinisikap naming maging tumpak, pakitandaan na ang mga awtomatikong pagsasalin ay maaaring maglaman ng mga pagkakamali o hindi pagkakatugma. Ang orihinal na dokumento sa orihinal nitong wika ang dapat ituring na opisyal na sanggunian. Para sa mahalagang impormasyon, inirerekomenda ang propesyonal na pagsasalin ng tao. Hindi kami mananagot sa anumang hindi pagkakaunawaan o maling interpretasyon na dulot ng paggamit ng pagsasaling ito.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->