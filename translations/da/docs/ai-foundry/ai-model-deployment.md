<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "2432e08775264e481d86a2e0e512a347",
  "translation_date": "2025-11-21T09:25:46+00:00",
  "source_file": "docs/ai-foundry/ai-model-deployment.md",
  "language_code": "da"
}
-->
# Udrulning af AI-modeller med Azure Developer CLI

**Kapiteloversigt:**
- **游닄 Kursushjem**: [AZD For Begyndere](../../README.md)
- **游닀 Nuv칝rende Kapitel**: Kapitel 2 - AI-First Udvikling
- **拘勇 Forrige**: [Microsoft Foundry Integration](microsoft-foundry-integration.md)
- **俱뫮잺 N칝ste**: [AI Workshop Lab](ai-workshop-lab.md)
- **游 N칝ste Kapitel**: [Kapitel 3: Konfiguration](../getting-started/configuration.md)

Denne vejledning giver omfattende instruktioner til udrulning af AI-modeller ved hj칝lp af AZD-skabeloner, og d칝kker alt fra modelvalg til produktionsudrulningsm칮nstre.

## Indholdsfortegnelse

- [Strategi for Modelvalg](../../../../docs/ai-foundry)
- [AZD-konfiguration for AI-modeller](../../../../docs/ai-foundry)
- [Udrulningsm칮nstre](../../../../docs/ai-foundry)
- [Modelh친ndtering](../../../../docs/ai-foundry)
- [Overvejelser i Produktion](../../../../docs/ai-foundry)
- [Overv친gning og Observabilitet](../../../../docs/ai-foundry)

## Strategi for Modelvalg

### Azure OpenAI Modeller

V칝lg den rette model til din brugssituation:

```yaml
# azure.yaml - Model configuration
services:
  ai-service:
    project: ./infra
    host: containerapp
    config:
      AZURE_OPENAI_MODELS: |
        [
          {
            "name": "gpt-4o-mini",
            "version": "2024-07-18",
            "deployment": "gpt-4o-mini",
            "capacity": 10,
            "format": "OpenAI"
          },
          {
            "name": "text-embedding-ada-002",
            "version": "2",
            "deployment": "text-embedding-ada-002", 
            "capacity": 30,
            "format": "OpenAI"
          }
        ]
```

### Kapacitetsplanl칝gning for Modeller

| Modeltype | Brugssituation | Anbefalet Kapacitet | Omkostningsovervejelser |
|-----------|----------------|---------------------|--------------------------|
| GPT-4o-mini | Chat, Q&A | 10-50 TPM | Omkostningseffektiv for de fleste arbejdsbelastninger |
| GPT-4 | Kompleks r칝sonnement | 20-100 TPM | H칮jere omkostninger, brug til premiumfunktioner |
| Text-embedding-ada-002 | S칮gning, RAG | 30-120 TPM | Essentiel for semantisk s칮gning |
| Whisper | Tale-til-tekst | 10-50 TPM | Arbejdsbelastninger med lydbehandling |

## AZD-konfiguration for AI-modeller

### Bicep-skabelonkonfiguration

Opret modeludrulninger via Bicep-skabeloner:

```bicep
// infra/main.bicep
@description('OpenAI model deployments')
param openAiModelDeployments array = [
  {
    name: 'gpt-4o-mini'
    model: {
      format: 'OpenAI'
      name: 'gpt-4o-mini'
      version: '2024-07-18'
    }
    sku: {
      name: 'Standard'
      capacity: 10
    }
  }
  {
    name: 'text-embedding-ada-002'
    model: {
      format: 'OpenAI'
      name: 'text-embedding-ada-002'
      version: '2'
    }
    sku: {
      name: 'Standard'
      capacity: 30
    }
  }
]

resource openAi 'Microsoft.CognitiveServices/accounts@2023-05-01' = {
  name: openAiAccountName
  location: location
  kind: 'OpenAI'
  properties: {
    customSubDomainName: openAiAccountName
    networkAcls: {
      defaultAction: 'Allow'
    }
    publicNetworkAccess: 'Enabled'
  }
  sku: {
    name: 'S0'
  }
}

@batchSize(1)
resource deployment 'Microsoft.CognitiveServices/accounts/deployments@2023-05-01' = [for deployment in openAiModelDeployments: {
  parent: openAi
  name: deployment.name
  properties: {
    model: deployment.model
  }
  sku: deployment.sku
}]
```

### Milj칮variabler

Konfigurer din applikationsmilj칮:

```bash
# .env konfiguration
AZURE_OPENAI_ENDPOINT=https://your-openai-resource.openai.azure.com/
AZURE_OPENAI_API_VERSION=2024-02-15-preview
AZURE_OPENAI_CHAT_DEPLOYMENT=gpt-4o-mini
AZURE_OPENAI_EMBED_DEPLOYMENT=text-embedding-ada-002
```

## Udrulningsm칮nstre

### M칮nster 1: Udrulning i 칠n region

```yaml
# azure.yaml - Single region
services:
  ai-app:
    project: ./src
    host: containerapp
    config:
      AZURE_OPENAI_ENDPOINT: ${AZURE_OPENAI_ENDPOINT}
      AZURE_OPENAI_CHAT_DEPLOYMENT: gpt-4o-mini
```

Bedst til:
- Udvikling og test
- Applikationer til 칠t marked
- Omkostningsoptimering

### M칮nster 2: Udrulning i flere regioner

```bicep
// Multi-region deployment
param regions array = ['eastus2', 'westus2', 'francecentral']

resource openAiMultiRegion 'Microsoft.CognitiveServices/accounts@2023-05-01' = [for region in regions: {
  name: '${openAiAccountName}-${region}'
  location: region
  // ... configuration
}]
```

Bedst til:
- Globale applikationer
- Krav om h칮j tilg칝ngelighed
- Belastningsfordeling

### M칮nster 3: Hybridudrulning

Kombiner Azure OpenAI med andre AI-tjenester:

```bicep
// Hybrid AI services
resource cognitiveServices 'Microsoft.CognitiveServices/accounts@2023-05-01' = {
  name: cognitiveServicesName
  location: location
  kind: 'CognitiveServices'
  properties: {
    customSubDomainName: cognitiveServicesName
  }
  sku: {
    name: 'S0'
  }
}

resource documentIntelligence 'Microsoft.CognitiveServices/accounts@2023-05-01' = {
  name: documentIntelligenceName
  location: location
  kind: 'FormRecognizer'
  properties: {
    customSubDomainName: documentIntelligenceName
  }
  sku: {
    name: 'S0'
  }
}
```

## Modelh친ndtering

### Versionskontrol

Spor modelversioner i din AZD-konfiguration:

```json
{
  "models": {
    "chat": {
      "name": "gpt-4o-mini",
      "version": "2024-07-18",
      "fallback": "gpt-35-turbo"
    },
    "embedding": {
      "name": "text-embedding-ada-002",
      "version": "2"
    }
  }
}
```

### Modelopdateringer

Brug AZD-hooks til modelopdateringer:

```bash
#!/bin/bash
# hooks/predeploy.sh

echo "Checking model availability..."
az cognitiveservices account list-models \
  --name $AZURE_OPENAI_ACCOUNT_NAME \
  --resource-group $AZURE_RESOURCE_GROUP \
  --query "[?name=='gpt-4o-mini']"
```

### A/B-test

Udrul flere modelversioner:

```bicep
param enableABTesting bool = false

resource chatDeployment 'Microsoft.CognitiveServices/accounts/deployments@2023-05-01' = {
  parent: openAi
  name: 'gpt-4o-mini-${enableABTesting ? 'v1' : 'prod'}'
  properties: {
    model: {
      format: 'OpenAI'
      name: 'gpt-4o-mini'
      version: '2024-07-18'
    }
  }
  sku: {
    name: 'Standard'
    capacity: enableABTesting ? 5 : 10
  }
}
```

## Overvejelser i Produktion

### Kapacitetsplanl칝gning

Beregn n칮dvendig kapacitet baseret p친 brugsm칮nstre:

```python
# Kapacitetsberegningseksempel
def calculate_required_capacity(
    requests_per_minute: int,
    avg_prompt_tokens: int,
    avg_completion_tokens: int,
    safety_margin: float = 0.2
) -> int:
    """Calculate required TPM capacity."""
    total_tokens_per_request = avg_prompt_tokens + avg_completion_tokens
    total_tpm = requests_per_minute * total_tokens_per_request
    return int(total_tpm * (1 + safety_margin))

# Eksempel p친 anvendelse
required_capacity = calculate_required_capacity(
    requests_per_minute=10,
    avg_prompt_tokens=500,
    avg_completion_tokens=200,
    safety_margin=0.3
)
print(f"Required capacity: {required_capacity} TPM")
```

### Auto-skalering Konfiguration

Konfigurer auto-skalering for Container Apps:

```bicep
resource containerApp 'Microsoft.App/containerApps@2024-03-01' = {
  name: containerAppName
  properties: {
    template: {
      scale: {
        minReplicas: 1
        maxReplicas: 10
        rules: [
          {
            name: 'http-rule'
            http: {
              metadata: {
                concurrentRequests: '10'
              }
            }
          }
          {
            name: 'cpu-rule'
            custom: {
              type: 'cpu'
              metadata: {
                type: 'Utilization'
                value: '70'
              }
            }
          }
        ]
      }
    }
  }
}
```

### Omkostningsoptimering

Implementer omkostningskontroller:

```bicep
@description('Enable cost management alerts')
param enableCostAlerts bool = true

resource budgetAlert 'Microsoft.Consumption/budgets@2023-05-01' = if (enableCostAlerts) {
  name: 'ai-budget-alert'
  properties: {
    timePeriod: {
      startDate: '2024-01-01'
      endDate: '2024-12-31'
    }
    timeGrain: 'Monthly'
    amount: 1000
    category: 'Cost'
    notifications: {
      Actual_GreaterThan_80_Percent: {
        enabled: true
        operator: 'GreaterThan'
        threshold: 80
        contactEmails: [
          'admin@yourcompany.com'
        ]
      }
    }
  }
}
```

## Overv친gning og Observabilitet

### Integration med Application Insights

Konfigurer overv친gning for AI-arbejdsbelastninger:

```bicep
resource applicationInsights 'Microsoft.Insights/components@2020-02-02' = {
  name: applicationInsightsName
  location: location
  kind: 'web'
  properties: {
    Application_Type: 'web'
    WorkspaceResourceId: logAnalyticsWorkspace.id
  }
}

// Custom metrics for AI models
resource aiMetrics 'Microsoft.Insights/components/analyticsItems@2020-02-02' = {
  parent: applicationInsights
  name: 'ai-model-metrics'
  properties: {
    content: '''
      customEvents
      | where name == "AI_Model_Request"
      | extend model = tostring(customDimensions.model)
      | extend tokens = toint(customDimensions.tokens)
      | extend latency = toint(customDimensions.latency_ms)
      | summarize 
          requests = count(),
          avg_tokens = avg(tokens),
          avg_latency = avg(latency)
        by model, bin(timestamp, 5m)
    '''
    type: 'query'
    scope: 'shared'
  }
}
```

### Brugerdefinerede Metrikker

Spor AI-specifikke metrikker:

```python
# Brugerdefineret telemetri for AI-modeller
import logging
from applicationinsights import TelemetryClient

class AITelemetry:
    def __init__(self, instrumentation_key: str):
        self.client = TelemetryClient(instrumentation_key)
    
    def track_model_request(self, model: str, tokens: int, latency_ms: int, success: bool):
        """Track AI model request metrics."""
        self.client.track_event(
            'AI_Model_Request',
            {
                'model': model,
                'tokens': str(tokens),
                'latency_ms': str(latency_ms),
                'success': str(success)
            }
        )
        
    def track_model_error(self, model: str, error_type: str, error_message: str):
        """Track AI model errors."""
        self.client.track_exception(
            type=error_type,
            value=error_message,
            properties={
                'model': model,
                'component': 'ai_model'
            }
        )
```

### Sundhedstjek

Implementer sundhedsoverv친gning for AI-tjenester:

```python
# Sundhedstjek-endepunkter
from fastapi import FastAPI, HTTPException
import httpx

app = FastAPI()

@app.get("/health/ai-models")
async def check_ai_models():
    """Check AI model availability."""
    try:
        # Test OpenAI-forbindelse
        async with httpx.AsyncClient() as client:
            response = await client.get(
                f"{AZURE_OPENAI_ENDPOINT}/openai/deployments",
                headers={"api-key": AZURE_OPENAI_API_KEY}
            )
            
        if response.status_code == 200:
            return {"status": "healthy", "models": response.json()}
        else:
            raise HTTPException(status_code=503, detail="AI models unavailable")
            
    except Exception as e:
        raise HTTPException(status_code=503, detail=f"Health check failed: {str(e)}")
```

## N칝ste Skridt

1. **Gennemg친 [Microsoft Foundry Integration Guide](microsoft-foundry-integration.md)** for m칮nstre til serviceintegration
2. **Fuldf칮r [AI Workshop Lab](ai-workshop-lab.md)** for praktisk erfaring
3. **Implementer [Produktions-AI-praksis](production-ai-practices.md)** for virksomhedsudrulninger
4. **Udforsk [AI Fejlfindingsguide](../troubleshooting/ai-troubleshooting.md)** for almindelige problemer

## Ressourcer

- [Azure OpenAI Modeltilg칝ngelighed](https://learn.microsoft.com/azure/ai-services/openai/concepts/models)
- [Azure Developer CLI Dokumentation](https://learn.microsoft.com/azure/developer/azure-developer-cli/)
- [Container Apps Skalering](https://learn.microsoft.com/azure/container-apps/scale-app)
- [AI Model Omkostningsoptimering](https://learn.microsoft.com/azure/ai-services/openai/how-to/manage-costs)

---

**Kapiteloversigt:**
- **游닄 Kursushjem**: [AZD For Begyndere](../../README.md)
- **游닀 Nuv칝rende Kapitel**: Kapitel 2 - AI-First Udvikling
- **拘勇 Forrige**: [Microsoft Foundry Integration](microsoft-foundry-integration.md)
- **俱뫮잺 N칝ste**: [AI Workshop Lab](ai-workshop-lab.md)
- **游 N칝ste Kapitel**: [Kapitel 3: Konfiguration](../getting-started/configuration.md)

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**Ansvarsfraskrivelse**:  
Dette dokument er blevet oversat ved hj칝lp af AI-overs칝ttelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selvom vi bestr칝ber os p친 n칮jagtighed, skal det bem칝rkes, at automatiserede overs칝ttelser kan indeholde fejl eller un칮jagtigheder. Det originale dokument p친 dets oprindelige sprog b칮r betragtes som den autoritative kilde. For kritisk information anbefales professionel menneskelig overs칝ttelse. Vi er ikke ansvarlige for eventuelle misforst친elser eller fejltolkninger, der opst친r som f칮lge af brugen af denne overs칝ttelse.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->