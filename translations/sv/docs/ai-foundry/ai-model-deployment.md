<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "6af361e2339c27aa56a9196e11b32cb7",
  "translation_date": "2025-09-17T23:13:35+00:00",
  "source_file": "docs/ai-foundry/ai-model-deployment.md",
  "language_code": "sv"
}
-->
# AI-modellimplementering med Azure Developer CLI

**Kapitelnavigation:**
- **游닄 Kurshem**: [AZD F칬r Nyb칬rjare](../../README.md)
- **游닀 Nuvarande Kapitel**: Kapitel 2 - AI-Driven Utveckling
- **拘勇 F칬reg친ende**: [Azure AI Foundry Integration](azure-ai-foundry-integration.md)
- **俱뫮잺 N칛sta**: [AI Workshop Lab](ai-workshop-lab.md)
- **游 N칛sta Kapitel**: [Kapitel 3: Konfiguration](../getting-started/configuration.md)

Den h칛r guiden ger omfattande instruktioner f칬r att implementera AI-modeller med AZD-mallar, och t칛cker allt fr친n modellval till produktionsimplementeringsm칬nster.

## Inneh친llsf칬rteckning

- [Strategi f칬r Modellval](../../../../docs/ai-foundry)
- [AZD-konfiguration f칬r AI-modeller](../../../../docs/ai-foundry)
- [Implementeringsm칬nster](../../../../docs/ai-foundry)
- [Modellhantering](../../../../docs/ai-foundry)
- [Produktions칬verv칛ganden](../../../../docs/ai-foundry)
- [칐vervakning och Observabilitet](../../../../docs/ai-foundry)

## Strategi f칬r Modellval

### Azure OpenAI-modeller

V칛lj r칛tt modell f칬r ditt anv칛ndningsomr친de:

```yaml
# azure.yaml - Model configuration
services:
  ai-service:
    project: ./infra
    host: containerapp
    config:
      AZURE_OPENAI_MODELS: |
        [
          {
            "name": "gpt-4o-mini",
            "version": "2024-07-18",
            "deployment": "gpt-4o-mini",
            "capacity": 10,
            "format": "OpenAI"
          },
          {
            "name": "text-embedding-ada-002",
            "version": "2",
            "deployment": "text-embedding-ada-002", 
            "capacity": 30,
            "format": "OpenAI"
          }
        ]
```

### Kapacitetsplanering f칬r Modeller

| Modelltyp | Anv칛ndningsomr친de | Rekommenderad Kapacitet | Kostnads칬verv칛ganden |
|-----------|-------------------|-------------------------|-----------------------|
| GPT-4o-mini | Chatt, Q&A | 10-50 TPM | Kostnadseffektivt f칬r de flesta arbetsbelastningar |
| GPT-4 | Komplex resonemang | 20-100 TPM | H칬gre kostnad, anv칛nd f칬r premiumfunktioner |
| Text-embedding-ada-002 | S칬k, RAG | 30-120 TPM | N칬dv칛ndig f칬r semantisk s칬kning |
| Whisper | Tal-till-text | 10-50 TPM | Arbetsbelastningar f칬r ljudbearbetning |

## AZD-konfiguration f칬r AI-modeller

### Bicep-mallkonfiguration

Skapa modellimplementeringar med Bicep-mallar:

```bicep
// infra/main.bicep
@description('OpenAI model deployments')
param openAiModelDeployments array = [
  {
    name: 'gpt-4o-mini'
    model: {
      format: 'OpenAI'
      name: 'gpt-4o-mini'
      version: '2024-07-18'
    }
    sku: {
      name: 'Standard'
      capacity: 10
    }
  }
  {
    name: 'text-embedding-ada-002'
    model: {
      format: 'OpenAI'
      name: 'text-embedding-ada-002'
      version: '2'
    }
    sku: {
      name: 'Standard'
      capacity: 30
    }
  }
]

resource openAi 'Microsoft.CognitiveServices/accounts@2023-05-01' = {
  name: openAiAccountName
  location: location
  kind: 'OpenAI'
  properties: {
    customSubDomainName: openAiAccountName
    networkAcls: {
      defaultAction: 'Allow'
    }
    publicNetworkAccess: 'Enabled'
  }
  sku: {
    name: 'S0'
  }
}

@batchSize(1)
resource deployment 'Microsoft.CognitiveServices/accounts/deployments@2023-05-01' = [for deployment in openAiModelDeployments: {
  parent: openAi
  name: deployment.name
  properties: {
    model: deployment.model
  }
  sku: deployment.sku
}]
```

### Milj칬variabler

Konfigurera din applikationsmilj칬:

```bash
# .env configuration
AZURE_OPENAI_ENDPOINT=https://your-openai-resource.openai.azure.com/
AZURE_OPENAI_API_VERSION=2024-02-15-preview
AZURE_OPENAI_CHAT_DEPLOYMENT=gpt-4o-mini
AZURE_OPENAI_EMBED_DEPLOYMENT=text-embedding-ada-002
```

## Implementeringsm칬nster

### M칬nster 1: Implementering i en region

```yaml
# azure.yaml - Single region
services:
  ai-app:
    project: ./src
    host: containerapp
    config:
      AZURE_OPENAI_ENDPOINT: ${AZURE_OPENAI_ENDPOINT}
      AZURE_OPENAI_CHAT_DEPLOYMENT: gpt-4o-mini
```

Passar b칛st f칬r:
- Utveckling och testning
- Applikationer f칬r enskilda marknader
- Kostnadsoptimering

### M칬nster 2: Implementering i flera regioner

```bicep
// Multi-region deployment
param regions array = ['eastus2', 'westus2', 'francecentral']

resource openAiMultiRegion 'Microsoft.CognitiveServices/accounts@2023-05-01' = [for region in regions: {
  name: '${openAiAccountName}-${region}'
  location: region
  // ... configuration
}]
```

Passar b칛st f칬r:
- Globala applikationer
- H칬ga krav p친 tillg칛nglighet
- Belastningsf칬rdelning

### M칬nster 3: Hybridimplementering

Kombinera Azure OpenAI med andra AI-tj칛nster:

```bicep
// Hybrid AI services
resource cognitiveServices 'Microsoft.CognitiveServices/accounts@2023-05-01' = {
  name: cognitiveServicesName
  location: location
  kind: 'CognitiveServices'
  properties: {
    customSubDomainName: cognitiveServicesName
  }
  sku: {
    name: 'S0'
  }
}

resource documentIntelligence 'Microsoft.CognitiveServices/accounts@2023-05-01' = {
  name: documentIntelligenceName
  location: location
  kind: 'FormRecognizer'
  properties: {
    customSubDomainName: documentIntelligenceName
  }
  sku: {
    name: 'S0'
  }
}
```

## Modellhantering

### Versionskontroll

Sp친ra modellversioner i din AZD-konfiguration:

```json
{
  "models": {
    "chat": {
      "name": "gpt-4o-mini",
      "version": "2024-07-18",
      "fallback": "gpt-35-turbo"
    },
    "embedding": {
      "name": "text-embedding-ada-002",
      "version": "2"
    }
  }
}
```

### Modelluppdateringar

Anv칛nd AZD-hooks f칬r modelluppdateringar:

```bash
#!/bin/bash
# hooks/predeploy.sh

echo "Checking model availability..."
az cognitiveservices account list-models \
  --name $AZURE_OPENAI_ACCOUNT_NAME \
  --resource-group $AZURE_RESOURCE_GROUP \
  --query "[?name=='gpt-4o-mini']"
```

### A/B-testning

Implementera flera modellversioner:

```bicep
param enableABTesting bool = false

resource chatDeployment 'Microsoft.CognitiveServices/accounts/deployments@2023-05-01' = {
  parent: openAi
  name: 'gpt-4o-mini-${enableABTesting ? 'v1' : 'prod'}'
  properties: {
    model: {
      format: 'OpenAI'
      name: 'gpt-4o-mini'
      version: '2024-07-18'
    }
  }
  sku: {
    name: 'Standard'
    capacity: enableABTesting ? 5 : 10
  }
}
```

## Produktions칬verv칛ganden

### Kapacitetsplanering

Ber칛kna n칬dv칛ndig kapacitet baserat p친 anv칛ndningsm칬nster:

```python
# Capacity calculation example
def calculate_required_capacity(
    requests_per_minute: int,
    avg_prompt_tokens: int,
    avg_completion_tokens: int,
    safety_margin: float = 0.2
) -> int:
    """Calculate required TPM capacity."""
    total_tokens_per_request = avg_prompt_tokens + avg_completion_tokens
    total_tpm = requests_per_minute * total_tokens_per_request
    return int(total_tpm * (1 + safety_margin))

# Example usage
required_capacity = calculate_required_capacity(
    requests_per_minute=10,
    avg_prompt_tokens=500,
    avg_completion_tokens=200,
    safety_margin=0.3
)
print(f"Required capacity: {required_capacity} TPM")
```

### Konfiguration f칬r Autoskalning

Konfigurera autoskalning f칬r Container Apps:

```bicep
resource containerApp 'Microsoft.App/containerApps@2024-03-01' = {
  name: containerAppName
  properties: {
    template: {
      scale: {
        minReplicas: 1
        maxReplicas: 10
        rules: [
          {
            name: 'http-rule'
            http: {
              metadata: {
                concurrentRequests: '10'
              }
            }
          }
          {
            name: 'cpu-rule'
            custom: {
              type: 'cpu'
              metadata: {
                type: 'Utilization'
                value: '70'
              }
            }
          }
        ]
      }
    }
  }
}
```

### Kostnadsoptimering

Implementera kostnadskontroller:

```bicep
@description('Enable cost management alerts')
param enableCostAlerts bool = true

resource budgetAlert 'Microsoft.Consumption/budgets@2023-05-01' = if (enableCostAlerts) {
  name: 'ai-budget-alert'
  properties: {
    timePeriod: {
      startDate: '2024-01-01'
      endDate: '2024-12-31'
    }
    timeGrain: 'Monthly'
    amount: 1000
    category: 'Cost'
    notifications: {
      Actual_GreaterThan_80_Percent: {
        enabled: true
        operator: 'GreaterThan'
        threshold: 80
        contactEmails: [
          'admin@yourcompany.com'
        ]
      }
    }
  }
}
```

## 칐vervakning och Observabilitet

### Integration med Application Insights

Konfigurera 칬vervakning f칬r AI-arbetsbelastningar:

```bicep
resource applicationInsights 'Microsoft.Insights/components@2020-02-02' = {
  name: applicationInsightsName
  location: location
  kind: 'web'
  properties: {
    Application_Type: 'web'
    WorkspaceResourceId: logAnalyticsWorkspace.id
  }
}

// Custom metrics for AI models
resource aiMetrics 'Microsoft.Insights/components/analyticsItems@2020-02-02' = {
  parent: applicationInsights
  name: 'ai-model-metrics'
  properties: {
    content: '''
      customEvents
      | where name == "AI_Model_Request"
      | extend model = tostring(customDimensions.model)
      | extend tokens = toint(customDimensions.tokens)
      | extend latency = toint(customDimensions.latency_ms)
      | summarize 
          requests = count(),
          avg_tokens = avg(tokens),
          avg_latency = avg(latency)
        by model, bin(timestamp, 5m)
    '''
    type: 'query'
    scope: 'shared'
  }
}
```

### Anpassade M칛tv칛rden

Sp친ra AI-specifika m칛tv칛rden:

```python
# Custom telemetry for AI models
import logging
from applicationinsights import TelemetryClient

class AITelemetry:
    def __init__(self, instrumentation_key: str):
        self.client = TelemetryClient(instrumentation_key)
    
    def track_model_request(self, model: str, tokens: int, latency_ms: int, success: bool):
        """Track AI model request metrics."""
        self.client.track_event(
            'AI_Model_Request',
            {
                'model': model,
                'tokens': str(tokens),
                'latency_ms': str(latency_ms),
                'success': str(success)
            }
        )
        
    def track_model_error(self, model: str, error_type: str, error_message: str):
        """Track AI model errors."""
        self.client.track_exception(
            type=error_type,
            value=error_message,
            properties={
                'model': model,
                'component': 'ai_model'
            }
        )
```

### H칛lsokontroller

Implementera 칬vervakning av AI-tj칛nstens h칛lsa:

```python
# Health check endpoints
from fastapi import FastAPI, HTTPException
import httpx

app = FastAPI()

@app.get("/health/ai-models")
async def check_ai_models():
    """Check AI model availability."""
    try:
        # Test OpenAI connection
        async with httpx.AsyncClient() as client:
            response = await client.get(
                f"{AZURE_OPENAI_ENDPOINT}/openai/deployments",
                headers={"api-key": AZURE_OPENAI_API_KEY}
            )
            
        if response.status_code == 200:
            return {"status": "healthy", "models": response.json()}
        else:
            raise HTTPException(status_code=503, detail="AI models unavailable")
            
    except Exception as e:
        raise HTTPException(status_code=503, detail=f"Health check failed: {str(e)}")
```

## N칛sta Steg

1. **Granska [Azure AI Foundry Integration Guide](azure-ai-foundry-integration.md)** f칬r m칬nster f칬r tj칛nsteintegration
2. **Slutf칬r [AI Workshop Lab](ai-workshop-lab.md)** f칬r praktisk erfarenhet
3. **Implementera [Produktions-AI-praktiker](production-ai-practices.md)** f칬r f칬retagsimplementeringar
4. **Utforska [AI Troubleshooting Guide](../troubleshooting/ai-troubleshooting.md)** f칬r vanliga problem

## Resurser

- [Tillg칛nglighet f칬r Azure OpenAI-modeller](https://learn.microsoft.com/azure/ai-services/openai/concepts/models)
- [Dokumentation f칬r Azure Developer CLI](https://learn.microsoft.com/azure/developer/azure-developer-cli/)
- [Skalning av Container Apps](https://learn.microsoft.com/azure/container-apps/scale-app)
- [Kostnadsoptimering f칬r AI-modeller](https://learn.microsoft.com/azure/ai-services/openai/how-to/manage-costs)

---

**Kapitelnavigation:**
- **游닄 Kurshem**: [AZD F칬r Nyb칬rjare](../../README.md)
- **游닀 Nuvarande Kapitel**: Kapitel 2 - AI-Driven Utveckling
- **拘勇 F칬reg친ende**: [Azure AI Foundry Integration](azure-ai-foundry-integration.md)
- **俱뫮잺 N칛sta**: [AI Workshop Lab](ai-workshop-lab.md)
- **游 N칛sta Kapitel**: [Kapitel 3: Konfiguration](../getting-started/configuration.md)

---

**Ansvarsfriskrivning**:  
Detta dokument har 칬versatts med hj칛lp av AI-칬vers칛ttningstj칛nsten [Co-op Translator](https://github.com/Azure/co-op-translator). 츿ven om vi str칛var efter noggrannhet, b칬r det noteras att automatiserade 칬vers칛ttningar kan inneh친lla fel eller brister. Det ursprungliga dokumentet p친 dess originalspr친k b칬r betraktas som den auktoritativa k칛llan. F칬r kritisk information rekommenderas professionell m칛nsklig 칬vers칛ttning. Vi ansvarar inte f칬r eventuella missf칬rst친nd eller feltolkningar som uppst친r vid anv칛ndning av denna 칬vers칛ttning.