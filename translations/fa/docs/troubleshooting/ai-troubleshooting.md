<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "c8ab8fd8ed338b3ec17484b453dcda68",
  "translation_date": "2025-09-17T16:25:55+00:00",
  "source_file": "docs/troubleshooting/ai-troubleshooting.md",
  "language_code": "fa"
}
-->
# Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ Ø¹ÛŒØ¨â€ŒÛŒØ§Ø¨ÛŒ ÙˆÛŒÚ˜Ù‡ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ

**Ù†Ø§ÙˆØ¨Ø±ÛŒ ÙØµÙ„:**
- **ðŸ“š ØµÙØ­Ù‡ Ø§ØµÙ„ÛŒ Ø¯ÙˆØ±Ù‡**: [AZD Ø¨Ø±Ø§ÛŒ Ù…Ø¨ØªØ¯ÛŒØ§Ù†](../../README.md)
- **ðŸ“– ÙØµÙ„ ÙØ¹Ù„ÛŒ**: ÙØµÙ„ Û· - Ø¹ÛŒØ¨â€ŒÛŒØ§Ø¨ÛŒ Ùˆ Ø¯ÛŒØ¨Ø§Ú¯
- **â¬…ï¸ Ù‚Ø¨Ù„ÛŒ**: [Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ Ø¯ÛŒØ¨Ø§Ú¯](debugging.md)
- **âž¡ï¸ ÙØµÙ„ Ø¨Ø¹Ø¯ÛŒ**: [ÙØµÙ„ Û¸: Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ Ùˆ Ø³Ø§Ø²Ù…Ø§Ù†ÛŒ](../ai-foundry/production-ai-practices.md)
- **ðŸ¤– Ù…Ø±ØªØ¨Ø·**: [ÙØµÙ„ Û²: ØªÙˆØ³Ø¹Ù‡ Ù…Ø¨ØªÙ†ÛŒ Ø¨Ø± Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ](../ai-foundry/azure-ai-foundry-integration.md)

**Ù‚Ø¨Ù„ÛŒ:** [Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ](../ai-foundry/production-ai-practices.md) | **Ø¨Ø¹Ø¯ÛŒ:** [Ø´Ø±ÙˆØ¹ Ú©Ø§Ø± Ø¨Ø§ AZD](../getting-started/README.md)

Ø§ÛŒÙ† Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ Ø¬Ø§Ù…Ø¹ Ø¨Ù‡ Ù…Ø´Ú©Ù„Ø§Øª Ø±Ø§ÛŒØ¬ Ø¯Ø± Ù‡Ù†Ú¯Ø§Ù… Ø§Ø³ØªÙ‚Ø±Ø§Ø± Ø±Ø§Ù‡Ú©Ø§Ø±Ù‡Ø§ÛŒ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ø¨Ø§ AZD Ù…ÛŒâ€ŒÙ¾Ø±Ø¯Ø§Ø²Ø¯ Ùˆ Ø±Ø§Ù‡â€ŒØ­Ù„â€ŒÙ‡Ø§ Ùˆ ØªÚ©Ù†ÛŒÚ©â€ŒÙ‡Ø§ÛŒ Ø¯ÛŒØ¨Ø§Ú¯ Ù…Ø®ØµÙˆØµ Ø®Ø¯Ù…Ø§Øª Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Azure Ø±Ø§ Ø§Ø±Ø§Ø¦Ù‡ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯.

## ÙÙ‡Ø±Ø³Øª Ù…Ø·Ø§Ù„Ø¨

- [Ù…Ø´Ú©Ù„Ø§Øª Ø³Ø±ÙˆÛŒØ³ Azure OpenAI](../../../../docs/troubleshooting)
- [Ù…Ø´Ú©Ù„Ø§Øª Ø¬Ø³ØªØ¬ÙˆÛŒ Azure AI](../../../../docs/troubleshooting)
- [Ù…Ø´Ú©Ù„Ø§Øª Ø§Ø³ØªÙ‚Ø±Ø§Ø± Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒÙ‡Ø§ÛŒ Ú©Ø§Ù†ØªÛŒÙ†Ø±ÛŒ](../../../../docs/troubleshooting)
- [Ø®Ø·Ø§Ù‡Ø§ÛŒ Ø§Ø­Ø±Ø§Ø² Ù‡ÙˆÛŒØª Ùˆ Ù…Ø¬ÙˆØ²Ù‡Ø§](../../../../docs/troubleshooting)
- [Ø´Ú©Ø³Øªâ€ŒÙ‡Ø§ÛŒ Ø§Ø³ØªÙ‚Ø±Ø§Ø± Ù…Ø¯Ù„](../../../../docs/troubleshooting)
- [Ù…Ø´Ú©Ù„Ø§Øª Ø¹Ù…Ù„Ú©Ø±Ø¯ Ùˆ Ù…Ù‚ÛŒØ§Ø³â€ŒÙ¾Ø°ÛŒØ±ÛŒ](../../../../docs/troubleshooting)
- [Ù…Ø¯ÛŒØ±ÛŒØª Ù‡Ø²ÛŒÙ†Ù‡ Ùˆ Ø³Ù‡Ù…ÛŒÙ‡](../../../../docs/troubleshooting)
- [Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§ Ùˆ ØªÚ©Ù†ÛŒÚ©â€ŒÙ‡Ø§ÛŒ Ø¯ÛŒØ¨Ø§Ú¯](../../../../docs/troubleshooting)

## Ù…Ø´Ú©Ù„Ø§Øª Ø³Ø±ÙˆÛŒØ³ Azure OpenAI

### Ù…Ø´Ú©Ù„: Ø³Ø±ÙˆÛŒØ³ OpenAI Ø¯Ø± Ù…Ù†Ø·Ù‚Ù‡ Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†ÛŒØ³Øª

**Ø¹Ù„Ø§Ø¦Ù…:**
```
Error: The requested resource type is not available in the location 'westus'
```

**Ø¯Ù„Ø§ÛŒÙ„:**
- Ø³Ø±ÙˆÛŒØ³ Azure OpenAI Ø¯Ø± Ù…Ù†Ø·Ù‚Ù‡ Ø§Ù†ØªØ®Ø§Ø¨ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ù†ÛŒØ³Øª
- Ø³Ù‡Ù…ÛŒÙ‡ Ø¯Ø± Ù…Ù†Ø§Ø·Ù‚ ØªØ±Ø¬ÛŒØ­ÛŒ ØªÙ…Ø§Ù… Ø´Ø¯Ù‡ Ø§Ø³Øª
- Ù…Ø­Ø¯ÙˆØ¯ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ø¸Ø±ÙÛŒØª Ù…Ù†Ø·Ù‚Ù‡â€ŒØ§ÛŒ

**Ø±Ø§Ù‡â€ŒØ­Ù„â€ŒÙ‡Ø§:**

1. **Ø¨Ø±Ø±Ø³ÛŒ Ø¯Ø³ØªØ±Ø³ÛŒ Ù…Ù†Ø·Ù‚Ù‡:**
```bash
# List available regions for OpenAI
az cognitiveservices account list-skus \
  --kind OpenAI \
  --query "[].locations[]" \
  --output table
```

2. **Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ ØªÙ†Ø¸ÛŒÙ…Ø§Øª AZD:**
```yaml
# azure.yaml - Force specific region
infra:
  provider: bicep
  path: infra
  module: main
parameters:
  location: "eastus2"  # Known working region
```

3. **Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ù†Ø§Ø·Ù‚ Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ†:**
```bicep
// infra/main.bicep - Multi-region fallback
@allowed([
  'eastus2'
  'francecentral'
  'canadaeast'
  'swedencentral'
])
param openAiLocation string = 'eastus2'
```

### Ù…Ø´Ú©Ù„: Ø³Ù‡Ù…ÛŒÙ‡ Ø§Ø³ØªÙ‚Ø±Ø§Ø± Ù…Ø¯Ù„ ØªÙ…Ø§Ù… Ø´Ø¯Ù‡ Ø§Ø³Øª

**Ø¹Ù„Ø§Ø¦Ù…:**
```
Error: Deployment failed due to insufficient quota
```

**Ø±Ø§Ù‡â€ŒØ­Ù„â€ŒÙ‡Ø§:**

1. **Ø¨Ø±Ø±Ø³ÛŒ Ø³Ù‡Ù…ÛŒÙ‡ ÙØ¹Ù„ÛŒ:**
```bash
# Check quota usage
az cognitiveservices usage list \
  --name YOUR_OPENAI_RESOURCE \
  --resource-group YOUR_RG
```

2. **Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø§ÙØ²Ø§ÛŒØ´ Ø³Ù‡Ù…ÛŒÙ‡:**
```bash
# Submit quota increase request
az support tickets create \
  --ticket-name "OpenAI Quota Increase" \
  --description "Need increased quota for production deployment" \
  --severity "minimal" \
  --problem-classification "/providers/Microsoft.Support/services/quota_service_guid/problemClassifications/quota_service_problemClassification_guid"
```

3. **Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¸Ø±ÙÛŒØª Ù…Ø¯Ù„:**
```bicep
// Reduce initial capacity
resource deployment 'Microsoft.CognitiveServices/accounts/deployments@2023-05-01' = {
  properties: {
    model: {
      format: 'OpenAI'
      name: 'gpt-4o-mini'
      version: '2024-07-18'
    }
  }
  sku: {
    name: 'Standard'
    capacity: 1  // Start with minimal capacity
  }
}
```

### Ù…Ø´Ú©Ù„: Ù†Ø³Ø®Ù‡ API Ù†Ø§Ù…Ø¹ØªØ¨Ø± Ø§Ø³Øª

**Ø¹Ù„Ø§Ø¦Ù…:**
```
Error: The API version '2023-05-15' is not available for OpenAI
```

**Ø±Ø§Ù‡â€ŒØ­Ù„â€ŒÙ‡Ø§:**

1. **Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù†Ø³Ø®Ù‡ Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒâ€ŒØ´Ø¯Ù‡ API:**
```python
# Use latest supported version
AZURE_OPENAI_API_VERSION = "2024-02-15-preview"
```

2. **Ø¨Ø±Ø±Ø³ÛŒ Ø³Ø§Ø²Ú¯Ø§Ø±ÛŒ Ù†Ø³Ø®Ù‡ API:**
```bash
# List supported API versions
az rest --method get \
  --url "https://management.azure.com/providers/Microsoft.CognitiveServices/operations?api-version=2023-05-01" \
  --query "value[?name.value=='Microsoft.CognitiveServices/accounts/read'].properties.serviceSpecification.metricSpecifications[].supportedApiVersions[]"
```

## Ù…Ø´Ú©Ù„Ø§Øª Ø¬Ø³ØªØ¬ÙˆÛŒ Azure AI

### Ù…Ø´Ú©Ù„: Ø³Ø·Ø­ Ù‚ÛŒÙ…Øªâ€ŒÚ¯Ø°Ø§Ø±ÛŒ Ø³Ø±ÙˆÛŒØ³ Ø¬Ø³ØªØ¬Ùˆ Ú©Ø§ÙÛŒ Ù†ÛŒØ³Øª

**Ø¹Ù„Ø§Ø¦Ù…:**
```
Error: Semantic search requires Basic tier or higher
```

**Ø±Ø§Ù‡â€ŒØ­Ù„â€ŒÙ‡Ø§:**

1. **Ø§Ø±ØªÙ‚Ø§ÛŒ Ø³Ø·Ø­ Ù‚ÛŒÙ…Øªâ€ŒÚ¯Ø°Ø§Ø±ÛŒ:**
```bicep
// infra/main.bicep - Use Basic tier
resource searchService 'Microsoft.Search/searchServices@2023-11-01' = {
  name: searchServiceName
  location: location
  sku: {
    name: 'basic'  // Minimum for semantic search
  }
  properties: {
    replicaCount: 1
    partitionCount: 1
    hostingMode: 'default'
    semanticSearch: 'standard'
  }
}
```

2. **ØºÛŒØ±ÙØ¹Ø§Ù„ Ú©Ø±Ø¯Ù† Ø¬Ø³ØªØ¬ÙˆÛŒ Ù…Ø¹Ù†Ø§ÛŒÛŒ (Ø¨Ø±Ø§ÛŒ ØªÙˆØ³Ø¹Ù‡):**
```bicep
// For development environments
resource searchService 'Microsoft.Search/searchServices@2023-11-01' = {
  name: searchServiceName
  sku: {
    name: 'free'
  }
  properties: {
    semanticSearch: 'disabled'
  }
}
```

### Ù…Ø´Ú©Ù„: Ø´Ú©Ø³Øª Ø¯Ø± Ø§ÛŒØ¬Ø§Ø¯ Ø§ÛŒÙ†Ø¯Ú©Ø³

**Ø¹Ù„Ø§Ø¦Ù…:**
```
Error: Cannot create index, insufficient permissions
```

**Ø±Ø§Ù‡â€ŒØ­Ù„â€ŒÙ‡Ø§:**

1. **Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù„ÛŒØ¯Ù‡Ø§ÛŒ Ø³Ø±ÙˆÛŒØ³ Ø¬Ø³ØªØ¬Ùˆ:**
```bash
# Get search service admin key
az search admin-key show \
  --service-name YOUR_SEARCH_SERVICE \
  --resource-group YOUR_RG
```

2. **Ø¨Ø±Ø±Ø³ÛŒ Ø·Ø±Ø­ Ø§ÛŒÙ†Ø¯Ú©Ø³:**
```python
# Validate index schema
from azure.search.documents.indexes import SearchIndexClient
from azure.search.documents.indexes.models import SearchIndex

def validate_index_schema(index_definition):
    """Validate index schema before creation."""
    required_fields = ['id', 'content']
    field_names = [field.name for field in index_definition.fields]
    
    for required in required_fields:
        if required not in field_names:
            raise ValueError(f"Missing required field: {required}")
```

3. **Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù‡ÙˆÛŒØª Ù…Ø¯ÛŒØ±ÛŒØªâ€ŒØ´Ø¯Ù‡:**
```bicep
// Grant search permissions to managed identity
resource searchContributor 'Microsoft.Authorization/roleAssignments@2022-04-01' = {
  scope: searchService
  name: guid(searchService.id, containerApp.id, searchIndexDataContributorRole)
  properties: {
    principalId: containerApp.identity.principalId
    roleDefinitionId: subscriptionResourceId('Microsoft.Authorization/roleDefinitions', '8ebe5a00-799e-43f5-93ac-243d3dce84a7')
    principalType: 'ServicePrincipal'
  }
}
```

## Ù…Ø´Ú©Ù„Ø§Øª Ø§Ø³ØªÙ‚Ø±Ø§Ø± Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒÙ‡Ø§ÛŒ Ú©Ø§Ù†ØªÛŒÙ†Ø±ÛŒ

### Ù…Ø´Ú©Ù„: Ø´Ú©Ø³Øª Ø¯Ø± Ø³Ø§Ø®Øª Ú©Ø§Ù†ØªÛŒÙ†Ø±

**Ø¹Ù„Ø§Ø¦Ù…:**
```
Error: Failed to build container image
```

**Ø±Ø§Ù‡â€ŒØ­Ù„â€ŒÙ‡Ø§:**

1. **Ø¨Ø±Ø±Ø³ÛŒ Ø³ÛŒÙ†ØªÚ©Ø³ Dockerfile:**
```dockerfile
# Dockerfile - Python AI app example
FROM python:3.11-slim

WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    gcc \
    && rm -rf /var/lib/apt/lists/*

# Copy requirements first for better caching
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

EXPOSE 8000
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
```

2. **Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ ÙˆØ§Ø¨Ø³ØªÚ¯ÛŒâ€ŒÙ‡Ø§:**
```txt
# requirements.txt - Pin versions for stability
fastapi==0.104.1
uvicorn==0.24.0
openai==1.3.7
azure-identity==1.14.1
azure-keyvault-secrets==4.7.0
azure-search-documents==11.4.0
azure-cosmos==4.5.1
```

3. **Ø§ÙØ²ÙˆØ¯Ù† Ø¨Ø±Ø±Ø³ÛŒ Ø³Ù„Ø§Ù…Øª:**
```python
# main.py - Add health check endpoint
from fastapi import FastAPI

app = FastAPI()

@app.get("/health")
async def health_check():
    return {"status": "healthy"}
```

### Ù…Ø´Ú©Ù„: Ø´Ú©Ø³Øª Ø¯Ø± Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø¨Ø±Ù†Ø§Ù…Ù‡ Ú©Ø§Ù†ØªÛŒÙ†Ø±ÛŒ

**Ø¹Ù„Ø§Ø¦Ù…:**
```
Error: Container failed to start within timeout period
```

**Ø±Ø§Ù‡â€ŒØ­Ù„â€ŒÙ‡Ø§:**

1. **Ø§ÙØ²Ø§ÛŒØ´ Ø²Ù…Ø§Ù† Ø§Ù†ØªØ¸Ø§Ø± Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ:**
```bicep
resource containerApp 'Microsoft.App/containerApps@2024-03-01' = {
  properties: {
    template: {
      containers: [
        {
          name: 'main'
          image: containerImage
          resources: {
            cpu: json('0.5')
            memory: '1Gi'
          }
          probes: [
            {
              type: 'startup'
              httpGet: {
                path: '/health'
                port: 8000
              }
              initialDelaySeconds: 30
              periodSeconds: 10
              timeoutSeconds: 5
              failureThreshold: 10  // Allow more time for AI models to load
            }
          ]
        }
      ]
    }
  }
}
```

2. **Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„:**
```python
# Lazy load models to reduce startup time
import asyncio
from contextlib import asynccontextmanager

class ModelManager:
    def __init__(self):
        self._client = None
        
    async def get_client(self):
        if self._client is None:
            self._client = await self._initialize_client()
        return self._client
        
    async def _initialize_client(self):
        # Initialize AI client here
        pass

@asynccontextmanager
async def lifespan(app: FastAPI):
    # Startup
    app.state.model_manager = ModelManager()
    yield
    # Shutdown
    pass

app = FastAPI(lifespan=lifespan)
```

## Ø®Ø·Ø§Ù‡Ø§ÛŒ Ø§Ø­Ø±Ø§Ø² Ù‡ÙˆÛŒØª Ùˆ Ù…Ø¬ÙˆØ²Ù‡Ø§

### Ù…Ø´Ú©Ù„: Ù…Ø¬ÙˆØ² Ù‡ÙˆÛŒØª Ù…Ø¯ÛŒØ±ÛŒØªâ€ŒØ´Ø¯Ù‡ Ø±Ø¯ Ø´Ø¯

**Ø¹Ù„Ø§Ø¦Ù…:**
```
Error: Authentication failed for Azure OpenAI Service
```

**Ø±Ø§Ù‡â€ŒØ­Ù„â€ŒÙ‡Ø§:**

1. **Ø¨Ø±Ø±Ø³ÛŒ ØªØ®ØµÛŒØµ Ù†Ù‚Ø´â€ŒÙ‡Ø§:**
```bash
# Check current role assignments
az role assignment list \
  --assignee YOUR_MANAGED_IDENTITY_ID \
  --scope /subscriptions/YOUR_SUBSCRIPTION/resourceGroups/YOUR_RG
```

2. **Ø§Ø®ØªØµØ§Øµ Ù†Ù‚Ø´â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯Ù†ÛŒØ§Ø²:**
```bicep
// Required role assignments for AI services
var cognitiveServicesOpenAIUserRole = subscriptionResourceId('Microsoft.Authorization/roleDefinitions', '5e0bd9bd-7b93-4f28-af87-19fc36ad61bd')
var searchIndexDataContributorRole = subscriptionResourceId('Microsoft.Authorization/roleDefinitions', '8ebe5a00-799e-43f5-93ac-243d3dce84a7')

resource openAiRoleAssignment 'Microsoft.Authorization/roleAssignments@2022-04-01' = {
  scope: openAi
  name: guid(openAi.id, containerApp.id, cognitiveServicesOpenAIUserRole)
  properties: {
    principalId: containerApp.identity.principalId
    roleDefinitionId: cognitiveServicesOpenAIUserRole
    principalType: 'ServicePrincipal'
  }
}
```

3. **ØªØ³Øª Ø§Ø­Ø±Ø§Ø² Ù‡ÙˆÛŒØª:**
```python
# Test managed identity authentication
from azure.identity import DefaultAzureCredential
from azure.core.exceptions import ClientAuthenticationError

async def test_authentication():
    try:
        credential = DefaultAzureCredential()
        token = await credential.get_token("https://cognitiveservices.azure.com/.default")
        print(f"Authentication successful: {token.token[:10]}...")
    except ClientAuthenticationError as e:
        print(f"Authentication failed: {e}")
```

### Ù…Ø´Ú©Ù„: Ø¯Ø³ØªØ±Ø³ÛŒ Ø¨Ù‡ Key Vault Ø±Ø¯ Ø´Ø¯

**Ø¹Ù„Ø§Ø¦Ù…:**
```
Error: The user, group or application does not have secrets get permission
```

**Ø±Ø§Ù‡â€ŒØ­Ù„â€ŒÙ‡Ø§:**

1. **Ø§Ø¹Ø·Ø§ÛŒ Ù…Ø¬ÙˆØ²Ù‡Ø§ÛŒ Key Vault:**
```bicep
resource keyVaultAccessPolicy 'Microsoft.KeyVault/vaults/accessPolicies@2023-07-01' = {
  parent: keyVault
  name: 'add'
  properties: {
    accessPolicies: [
      {
        tenantId: subscription().tenantId
        objectId: containerApp.identity.principalId
        permissions: {
          secrets: ['get', 'list']
        }
      }
    ]
  }
}
```

2. **Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² RBAC Ø¨Ù‡ Ø¬Ø§ÛŒ Ø³ÛŒØ§Ø³Øªâ€ŒÙ‡Ø§ÛŒ Ø¯Ø³ØªØ±Ø³ÛŒ:**
```bicep
resource keyVaultSecretsUserRole 'Microsoft.Authorization/roleAssignments@2022-04-01' = {
  scope: keyVault
  name: guid(keyVault.id, containerApp.id, 'Key Vault Secrets User')
  properties: {
    principalId: containerApp.identity.principalId
    roleDefinitionId: subscriptionResourceId('Microsoft.Authorization/roleDefinitions', '4633458b-17de-408a-b874-0445c86b69e6')
    principalType: 'ServicePrincipal'
  }
}
```

## Ø´Ú©Ø³Øªâ€ŒÙ‡Ø§ÛŒ Ø§Ø³ØªÙ‚Ø±Ø§Ø± Ù…Ø¯Ù„

### Ù…Ø´Ú©Ù„: Ù†Ø³Ø®Ù‡ Ù…Ø¯Ù„ Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†ÛŒØ³Øª

**Ø¹Ù„Ø§Ø¦Ù…:**
```
Error: Model version 'gpt-4-32k' is not available
```

**Ø±Ø§Ù‡â€ŒØ­Ù„â€ŒÙ‡Ø§:**

1. **Ø¨Ø±Ø±Ø³ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯:**
```bash
# List available models
az cognitiveservices account list-models \
  --name YOUR_OPENAI_RESOURCE \
  --resource-group YOUR_RG \
  --query "[].{name:model.name, version:model.version}" \
  --output table
```

2. **Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ†:**
```bicep
// Model deployment with fallback
@description('Primary model configuration')
param primaryModel object = {
  name: 'gpt-4o-mini'
  version: '2024-07-18'
}

@description('Fallback model configuration')
param fallbackModel object = {
  name: 'gpt-35-turbo'
  version: '0125'
}

// Try primary model first, fallback if unavailable
resource primaryDeployment 'Microsoft.CognitiveServices/accounts/deployments@2023-05-01' = {
  parent: openAi
  name: 'chat-model'
  properties: {
    model: primaryModel
  }
  sku: {
    name: 'Standard'
    capacity: 10
  }
}
```

3. **Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ Ù…Ø¯Ù„ Ù‚Ø¨Ù„ Ø§Ø² Ø§Ø³ØªÙ‚Ø±Ø§Ø±:**
```python
# Pre-deployment model validation
import httpx

async def validate_model_availability(model_name: str, version: str) -> bool:
    """Check if model is available before deployment."""
    try:
        async with httpx.AsyncClient() as client:
            response = await client.get(
                f"{AZURE_OPENAI_ENDPOINT}/openai/models",
                headers={"api-key": AZURE_OPENAI_API_KEY}
            )
            models = response.json()
            return any(
                model["id"] == f"{model_name}-{version}"
                for model in models.get("data", [])
            )
    except Exception:
        return False
```

## Ù…Ø´Ú©Ù„Ø§Øª Ø¹Ù…Ù„Ú©Ø±Ø¯ Ùˆ Ù…Ù‚ÛŒØ§Ø³â€ŒÙ¾Ø°ÛŒØ±ÛŒ

### Ù…Ø´Ú©Ù„: Ø²Ù…Ø§Ù† Ù¾Ø§Ø³Ø®â€ŒØ¯Ù‡ÛŒ Ø¨Ø§Ù„Ø§

**Ø¹Ù„Ø§Ø¦Ù…:**
- Ø²Ù…Ø§Ù† Ù¾Ø§Ø³Ø®â€ŒØ¯Ù‡ÛŒ > Û³Û° Ø«Ø§Ù†ÛŒÙ‡
- Ø®Ø·Ø§Ù‡Ø§ÛŒ ØªØ§ÛŒÙ…â€ŒØ§ÙˆØª
- ØªØ¬Ø±Ø¨Ù‡ Ú©Ø§Ø±Ø¨Ø±ÛŒ Ø¶Ø¹ÛŒÙ

**Ø±Ø§Ù‡â€ŒØ­Ù„â€ŒÙ‡Ø§:**

1. **Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ ØªØ§ÛŒÙ…â€ŒØ§ÙˆØª Ø¯Ø±Ø®ÙˆØ§Ø³Øªâ€ŒÙ‡Ø§:**
```python
# Configure proper timeouts
import httpx

client = httpx.AsyncClient(
    timeout=httpx.Timeout(
        connect=5.0,
        read=30.0,
        write=10.0,
        pool=10.0
    )
)
```

2. **Ø§ÙØ²ÙˆØ¯Ù† Ú©Ø´ Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§:**
```python
# Redis cache for responses
import redis.asyncio as redis
import json

class ResponseCache:
    def __init__(self, redis_url: str):
        self.redis = redis.from_url(redis_url)
        
    async def get_cached_response(self, query_hash: str) -> str | None:
        """Get cached response if available."""
        cached = await self.redis.get(f"ai_response:{query_hash}")
        return cached.decode() if cached else None
        
    async def cache_response(self, query_hash: str, response: str, ttl: int = 3600):
        """Cache AI response with TTL."""
        await self.redis.setex(f"ai_response:{query_hash}", ttl, response)
```

3. **Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ Ù…Ù‚ÛŒØ§Ø³â€ŒÚ¯Ø°Ø§Ø±ÛŒ Ø®ÙˆØ¯Ú©Ø§Ø±:**
```bicep
resource containerApp 'Microsoft.App/containerApps@2024-03-01' = {
  properties: {
    template: {
      scale: {
        minReplicas: 2
        maxReplicas: 20
        rules: [
          {
            name: 'http-requests'
            http: {
              metadata: {
                concurrentRequests: '5'  // Scale aggressively for AI workloads
              }
            }
          }
          {
            name: 'cpu-utilization'
            custom: {
              type: 'cpu'
              metadata: {
                type: 'Utilization'
                value: '60'  // Lower threshold for AI apps
              }
            }
          }
        ]
      }
    }
  }
}
```

### Ù…Ø´Ú©Ù„: Ø®Ø·Ø§Ù‡Ø§ÛŒ Ú©Ù…Ø¨ÙˆØ¯ Ø­Ø§ÙØ¸Ù‡

**Ø¹Ù„Ø§Ø¦Ù…:**
```
Error: Container killed due to memory limit exceeded
```

**Ø±Ø§Ù‡â€ŒØ­Ù„â€ŒÙ‡Ø§:**

1. **Ø§ÙØ²Ø§ÛŒØ´ ØªØ®ØµÛŒØµ Ø­Ø§ÙØ¸Ù‡:**
```bicep
resource containerApp 'Microsoft.App/containerApps@2024-03-01' = {
  properties: {
    template: {
      containers: [
        {
          name: 'main'
          resources: {
            cpu: json('1.0')
            memory: '2Gi'  // Increase for AI workloads
          }
        }
      ]
    }
  }
}
```

2. **Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø­Ø§ÙØ¸Ù‡:**
```python
# Memory-efficient model handling
import gc
import psutil

class MemoryOptimizedAI:
    def __init__(self):
        self.max_memory_percent = 80
        
    async def process_request(self, request):
        """Process request with memory monitoring."""
        # Check memory usage before processing
        memory_percent = psutil.virtual_memory().percent
        if memory_percent > self.max_memory_percent:
            gc.collect()  # Force garbage collection
            
        result = await self._process_ai_request(request)
        
        # Clean up after processing
        gc.collect()
        return result
```

## Ù…Ø¯ÛŒØ±ÛŒØª Ù‡Ø²ÛŒÙ†Ù‡ Ùˆ Ø³Ù‡Ù…ÛŒÙ‡

### Ù…Ø´Ú©Ù„: Ù‡Ø²ÛŒÙ†Ù‡â€ŒÙ‡Ø§ÛŒ ØºÛŒØ±Ù…Ù†ØªØ¸Ø±Ù‡ Ø¨Ø§Ù„Ø§

**Ø¹Ù„Ø§Ø¦Ù…:**
- ØµÙˆØ±ØªØ­Ø³Ø§Ø¨ Azure Ø¨ÛŒØ´ØªØ± Ø§Ø² Ø­Ø¯ Ø§Ù†ØªØ¸Ø§Ø±
- Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ØªÙˆÚ©Ù† Ø¨ÛŒØ´ Ø§Ø² ØªØ®Ù…ÛŒÙ†â€ŒÙ‡Ø§
- Ù‡Ø´Ø¯Ø§Ø±Ù‡Ø§ÛŒ Ø¨ÙˆØ¯Ø¬Ù‡ ÙØ¹Ø§Ù„ Ø´Ø¯Ù‡

**Ø±Ø§Ù‡â€ŒØ­Ù„â€ŒÙ‡Ø§:**

1. **Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ú©Ù†ØªØ±Ù„ Ù‡Ø²ÛŒÙ†Ù‡:**
```python
# Token usage tracking
class TokenTracker:
    def __init__(self, monthly_limit: int = 100000):
        self.monthly_limit = monthly_limit
        self.current_usage = 0
        
    async def track_usage(self, prompt_tokens: int, completion_tokens: int):
        """Track token usage with limits."""
        total_tokens = prompt_tokens + completion_tokens
        self.current_usage += total_tokens
        
        if self.current_usage > self.monthly_limit:
            raise Exception("Monthly token limit exceeded")
            
        return total_tokens
```

2. **ØªÙ†Ø¸ÛŒÙ… Ù‡Ø´Ø¯Ø§Ø±Ù‡Ø§ÛŒ Ù‡Ø²ÛŒÙ†Ù‡:**
```bicep
resource budgetAlert 'Microsoft.Consumption/budgets@2023-05-01' = {
  name: 'ai-workload-budget'
  properties: {
    timePeriod: {
      startDate: '2024-01-01'
      endDate: '2024-12-31'
    }
    timeGrain: 'Monthly'
    amount: 500  // $500 monthly limit
    category: 'Cost'
    notifications: {
      Actual_GreaterThan_80_Percent: {
        enabled: true
        operator: 'GreaterThan'
        threshold: 80
        contactEmails: ['admin@company.com']
        contactRoles: ['Owner']
      }
    }
  }
}
```

3. **Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø§Ù†ØªØ®Ø§Ø¨ Ù…Ø¯Ù„:**
```python
# Cost-aware model selection
MODEL_COSTS = {
    'gpt-4o-mini': 0.00015,  # per 1K tokens
    'gpt-4': 0.03,          # per 1K tokens
    'gpt-35-turbo': 0.0015  # per 1K tokens
}

def select_model_by_cost(complexity: str, budget_remaining: float) -> str:
    """Select model based on complexity and budget."""
    if complexity == 'simple' or budget_remaining < 10:
        return 'gpt-4o-mini'
    elif complexity == 'medium':
        return 'gpt-35-turbo'
    else:
        return 'gpt-4'
```

## Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§ Ùˆ ØªÚ©Ù†ÛŒÚ©â€ŒÙ‡Ø§ÛŒ Ø¯ÛŒØ¨Ø§Ú¯

### Ø¯Ø³ØªÙˆØ±Ø§Øª Ø¯ÛŒØ¨Ø§Ú¯ AZD

```bash
# Enable verbose logging
azd up --debug

# Check deployment status
azd show

# View deployment logs
azd logs --follow

# Check environment variables
azd env get-values
```

### Ø¯ÛŒØ¨Ø§Ú¯ Ø¨Ø±Ù†Ø§Ù…Ù‡

1. **Ù„Ø§Ú¯â€ŒØ¨Ø±Ø¯Ø§Ø±ÛŒ Ø³Ø§Ø®ØªØ§Ø±ÛŒØ§ÙØªÙ‡:**
```python
import logging
import json

# Configure structured logging for AI applications
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

logger = logging.getLogger(__name__)

def log_ai_request(model: str, tokens: int, latency: float, success: bool):
    """Log AI request details."""
    logger.info(json.dumps({
        'event': 'ai_request',
        'model': model,
        'tokens': tokens,
        'latency_ms': latency,
        'success': success
    }))
```

2. **Ù†Ù‚Ø§Ø· Ø§Ù†ØªÙ‡Ø§ÛŒÛŒ Ø¨Ø±Ø±Ø³ÛŒ Ø³Ù„Ø§Ù…Øª:**
```python
@app.get("/debug/health")
async def detailed_health_check():
    """Comprehensive health check for debugging."""
    checks = {}
    
    # Check OpenAI connectivity
    try:
        client = AsyncOpenAI(azure_endpoint=AZURE_OPENAI_ENDPOINT)
        await client.models.list()
        checks['openai'] = {'status': 'healthy'}
    except Exception as e:
        checks['openai'] = {'status': 'unhealthy', 'error': str(e)}
    
    # Check Search service
    try:
        search_client = SearchIndexClient(
            endpoint=AZURE_SEARCH_ENDPOINT,
            credential=DefaultAzureCredential()
        )
        indexes = await search_client.list_index_names()
        checks['search'] = {'status': 'healthy', 'indexes': list(indexes)}
    except Exception as e:
        checks['search'] = {'status': 'unhealthy', 'error': str(e)}
    
    return checks
```

3. **Ù†Ø¸Ø§Ø±Øª Ø¨Ø± Ø¹Ù…Ù„Ú©Ø±Ø¯:**
```python
import time
from functools import wraps

def monitor_performance(func):
    """Decorator to monitor function performance."""
    @wraps(func)
    async def wrapper(*args, **kwargs):
        start_time = time.time()
        try:
            result = await func(*args, **kwargs)
            success = True
        except Exception as e:
            result = None
            success = False
            raise
        finally:
            end_time = time.time()
            latency = (end_time - start_time) * 1000
            
            logger.info(json.dumps({
                'function': func.__name__,
                'latency_ms': latency,
                'success': success
            }))
        
        return result
    return wrapper
```

## Ú©Ø¯Ù‡Ø§ÛŒ Ø®Ø·Ø§ÛŒ Ø±Ø§ÛŒØ¬ Ùˆ Ø±Ø§Ù‡â€ŒØ­Ù„â€ŒÙ‡Ø§

| Ú©Ø¯ Ø®Ø·Ø§ | ØªÙˆØ¶ÛŒØ­Ø§Øª | Ø±Ø§Ù‡â€ŒØ­Ù„ |
|--------|---------|--------|
| 401 | ØºÛŒØ±Ù…Ø¬Ø§Ø² | Ú©Ù„ÛŒØ¯Ù‡Ø§ÛŒ API Ùˆ Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ Ù‡ÙˆÛŒØª Ù…Ø¯ÛŒØ±ÛŒØªâ€ŒØ´Ø¯Ù‡ Ø±Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒØ¯ |
| 403 | Ù…Ù…Ù†ÙˆØ¹ | ØªØ®ØµÛŒØµ Ù†Ù‚Ø´â€ŒÙ‡Ø§ÛŒ RBAC Ø±Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒØ¯ |
| 429 | Ù…Ø­Ø¯ÙˆØ¯ÛŒØª Ù†Ø±Ø® | Ù…Ù†Ø·Ù‚ ØªÙ„Ø§Ø´ Ù…Ø¬Ø¯Ø¯ Ø¨Ø§ backoff Ù†Ù…Ø§ÛŒÛŒ Ø±Ø§ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ú©Ù†ÛŒØ¯ |
| 500 | Ø®Ø·Ø§ÛŒ Ø¯Ø§Ø®Ù„ÛŒ Ø³Ø±ÙˆØ± | ÙˆØ¶Ø¹ÛŒØª Ø§Ø³ØªÙ‚Ø±Ø§Ø± Ù…Ø¯Ù„ Ùˆ Ù„Ø§Ú¯â€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒØ¯ |
| 503 | Ø³Ø±ÙˆÛŒØ³ Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†ÛŒØ³Øª | Ø³Ù„Ø§Ù…Øª Ø³Ø±ÙˆÛŒØ³ Ùˆ Ø¯Ø³ØªØ±Ø³ÛŒ Ù…Ù†Ø·Ù‚Ù‡â€ŒØ§ÛŒ Ø±Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒØ¯ |

## Ù…Ø±Ø§Ø­Ù„ Ø¨Ø¹Ø¯ÛŒ

1. **Ø¨Ø±Ø±Ø³ÛŒ [Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ Ø§Ø³ØªÙ‚Ø±Ø§Ø± Ù…Ø¯Ù„ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ](ai-model-deployment.md)** Ø¨Ø±Ø§ÛŒ Ø¨Ù‡ØªØ±ÛŒÙ† Ø´ÛŒÙˆÙ‡â€ŒÙ‡Ø§ÛŒ Ø§Ø³ØªÙ‚Ø±Ø§Ø±
2. **ØªÚ©Ù…ÛŒÙ„ [Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ](production-ai-practices.md)** Ø¨Ø±Ø§ÛŒ Ø±Ø§Ù‡Ú©Ø§Ø±Ù‡Ø§ÛŒ Ø¢Ù…Ø§Ø¯Ù‡ Ø³Ø§Ø²Ù…Ø§Ù†ÛŒ
3. **Ù¾ÛŒÙˆØ³ØªÙ† Ø¨Ù‡ [Ø¯ÛŒØ³Ú©ÙˆØ±Ø¯ Azure AI Foundry](https://aka.ms/foundry/discord)** Ø¨Ø±Ø§ÛŒ Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ø¬Ø§Ù…Ø¹Ù‡
4. **Ø§Ø±Ø³Ø§Ù„ Ù…Ø´Ú©Ù„Ø§Øª** Ø¨Ù‡ [Ù…Ø®Ø²Ù† GitHub AZD](https://github.com/Azure/azure-dev) Ø¨Ø±Ø§ÛŒ Ù…Ø´Ú©Ù„Ø§Øª Ø®Ø§Øµ AZD

## Ù…Ù†Ø§Ø¨Ø¹

- [Ø¹ÛŒØ¨â€ŒÛŒØ§Ø¨ÛŒ Ø³Ø±ÙˆÛŒØ³ Azure OpenAI](https://learn.microsoft.com/azure/ai-services/openai/troubleshooting)
- [Ø¹ÛŒØ¨â€ŒÛŒØ§Ø¨ÛŒ Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒÙ‡Ø§ÛŒ Ú©Ø§Ù†ØªÛŒÙ†Ø±ÛŒ](https://learn.microsoft.com/azure/container-apps/troubleshooting)
- [Ø¹ÛŒØ¨â€ŒÛŒØ§Ø¨ÛŒ Ø¬Ø³ØªØ¬ÙˆÛŒ Azure AI](https://learn.microsoft.com/azure/search/search-monitor-logs)

---

**Ù†Ø§ÙˆØ¨Ø±ÛŒ ÙØµÙ„:**
- **ðŸ“š ØµÙØ­Ù‡ Ø§ØµÙ„ÛŒ Ø¯ÙˆØ±Ù‡**: [AZD Ø¨Ø±Ø§ÛŒ Ù…Ø¨ØªØ¯ÛŒØ§Ù†](../../README.md)
- **ðŸ“– ÙØµÙ„ ÙØ¹Ù„ÛŒ**: ÙØµÙ„ Û· - Ø¹ÛŒØ¨â€ŒÛŒØ§Ø¨ÛŒ Ùˆ Ø¯ÛŒØ¨Ø§Ú¯
- **â¬…ï¸ Ù‚Ø¨Ù„ÛŒ**: [Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ Ø¯ÛŒØ¨Ø§Ú¯](debugging.md)
- **âž¡ï¸ ÙØµÙ„ Ø¨Ø¹Ø¯ÛŒ**: [ÙØµÙ„ Û¸: Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ Ùˆ Ø³Ø§Ø²Ù…Ø§Ù†ÛŒ](../ai-foundry/production-ai-practices.md)
- **ðŸ¤– Ù…Ø±ØªØ¨Ø·**: [ÙØµÙ„ Û²: ØªÙˆØ³Ø¹Ù‡ Ù…Ø¨ØªÙ†ÛŒ Ø¨Ø± Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ](../ai-foundry/azure-ai-foundry-integration.md)
- [Ø¹ÛŒØ¨â€ŒÛŒØ§Ø¨ÛŒ Azure Developer CLI](https://learn.microsoft.com/azure/developer/azure-developer-cli/troubleshoot)

---

**Ø³Ù„Ø¨ Ù…Ø³Ø¦ÙˆÙ„ÛŒØª**:  
Ø§ÛŒÙ† Ø³Ù†Ø¯ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø³Ø±ÙˆÛŒØ³ ØªØ±Ø¬Ù…Ù‡ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ [Co-op Translator](https://github.com/Azure/co-op-translator) ØªØ±Ø¬Ù…Ù‡ Ø´Ø¯Ù‡ Ø§Ø³Øª. Ø¯Ø± Ø­Ø§Ù„ÛŒ Ú©Ù‡ Ù…Ø§ ØªÙ„Ø§Ø´ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… Ø¯Ù‚Øª Ø±Ø§ Ø­ÙØ¸ Ú©Ù†ÛŒÙ…ØŒ Ù„Ø·ÙØ§Ù‹ ØªÙˆØ¬Ù‡ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´ÛŒØ¯ Ú©Ù‡ ØªØ±Ø¬Ù…Ù‡â€ŒÙ‡Ø§ÛŒ Ø®ÙˆØ¯Ú©Ø§Ø± Ù…Ù…Ú©Ù† Ø§Ø³Øª Ø´Ø§Ù…Ù„ Ø®Ø·Ø§Ù‡Ø§ ÛŒØ§ Ù†Ø§Ø¯Ø±Ø³ØªÛŒâ€ŒÙ‡Ø§ Ø¨Ø§Ø´Ù†Ø¯. Ø³Ù†Ø¯ Ø§ØµÙ„ÛŒ Ø¨Ù‡ Ø²Ø¨Ø§Ù† Ø§ØµÙ„ÛŒ Ø¢Ù† Ø¨Ø§ÛŒØ¯ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Ù…Ù†Ø¨Ø¹ Ù…Ø¹ØªØ¨Ø± Ø¯Ø± Ù†Ø¸Ø± Ú¯Ø±ÙØªÙ‡ Ø´ÙˆØ¯. Ø¨Ø±Ø§ÛŒ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø­Ø³Ø§Ø³ØŒ ØªÙˆØµÛŒÙ‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ø§Ø² ØªØ±Ø¬Ù…Ù‡ Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ Ø§Ù†Ø³Ø§Ù†ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯. Ù…Ø§ Ù…Ø³Ø¦ÙˆÙ„ÛŒØªÛŒ Ø¯Ø± Ù‚Ø¨Ø§Ù„ Ø³ÙˆØ¡ ØªÙØ§Ù‡Ù…â€ŒÙ‡Ø§ ÛŒØ§ ØªÙØ³ÛŒØ±Ù‡Ø§ÛŒ Ù†Ø§Ø¯Ø±Ø³Øª Ù†Ø§Ø´ÛŒ Ø§Ø² Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø§ÛŒÙ† ØªØ±Ø¬Ù…Ù‡ Ù†Ø¯Ø§Ø±ÛŒÙ….